\documentclass[12pt, letter paper]{article}
\usepackage[utf8]{inputenc} 

\begin{document}
	\section{Project 1}
	\subsection{Feature correlation}
	
	The spambase data set has as much as $57$ features, so elimination of some of those may have a significant impact on efficiency of classification algorithms. The most common approach here is to look at feature correlations. This way we can remove some variables that are highly correlated with others, as they do not provide much more information in the classification problem. It is also faster and more convenient to use only one of such variables. First let us take a look at correlation matrix, that was presented on $\textbf{ADD REF}$ as it can show some interesting details about the data set.
	
	$\textbf{paste correlation matrix here}$
	
	From the correlation matrix we see that there are a few features highly correlated with one another. However generally the explanatory variables don't seem to be highly correlated, so we expect that not many variables can be skipped in the later analysis. To check that further we use function $\texttt{findCorrelation}$ with cutoff equal to $0.8$.
	
	$\textbf{paste findCorrelation and results here}$
	
	From the output we can see that with adopted cutoff we can eliminate features 'num415' and 'num857', so now we are left with $55$ variables. This is not the only advantage that correlation matrix can give us, as we can also find variables that have the highest correlation with e-mail type. However earlier presented correlation matrix is not easy to read. Let us take out the features that have highest positive or negative correlation with type.
	
	$\textbf{paste correlation with spam here}$
	
	Results presented above are also visible on the matrix. It's important to point out that those features may have a great impact on classification of e-mails, which we will try to investigate later on.
	
	\subsection{Average frequency}
	
	Other important data feature is the average value. In the classification problem the average value among all variables is not as crucial as the difference in average word or character frequency between two investigated types. Let us take out variables, that demonstrate the highest difference in average value.
	
	$\textbf{paste average value for type here}$
	
	Here also we expect that, as for features highly correlated with spam, those also will have crucial impact on spam classification.
	
	\subsection{Discriminant analysis}
	
	\subsection*{All features}
	
	Another useful classification algorithm is discriminant analysis. In this section we will use linear and quadratic discriminant analysis to determine e-mail type on data transformed as earlier, so we compare scaled data and data transformed with logarithm. At first let us use these methods on the whole data set and examine the accuracy. To do it we use the confusion matrix, first for scaled data we have Table \ref{tab:confusion_matrix_lda1} and Table \ref{tab:confusion_matrix_qda1}.
	
	\begin{table}[h]
		\centering
		\begin{tabular}{lcc}
			& \textbf{Reference} & \\
			\textbf{Prediction} & nonspam & spam \\
			nonspam & 661 & 36 \\
			spam & 90 & 363 \\
		\end{tabular}
		\caption{Confusion Matrix - LDA, all features}
		\label{tab:confusion_matrix_lda1}
	\end{table}
	
	\begin{table}[h]
		\centering
		\begin{tabular}{lcc}
			& \textbf{Reference} & \\
			\textbf{Prediction} & nonspam & spam \\
			nonspam & 539 & 158 \\
			spam & 18 & 435 \\
		\end{tabular}
		\caption{Confusion Matrix - QDA, all features}
		\label{tab:confusion_matrix_qda1}
	\end{table}

	The accuracy for described methods is quite satisfying, for LDA it's $0.8904$ and for QDA - $0.847$. So in this case linear method works significantly better. There's also a disproportion in confusion matrices for those methods. For LDA there are more spam messages predicted as nonspam. Opposite situation occurs for QDA, which works in favor of spam e-mails. Those features are nicely represented with sensitivity and specificity, that for LDA are respectively $0.9098$, $0.8802$ and for QDA: $0.7336$, $0.9677$. 
	
	We perform the same algorithms on the data with logarithmic transformation and obtain Tables \ref{tab:confusion_matrix_lda1_log} and \ref{tab:confusion_matrix_qda1_log}.
	
	\begin{table}[h]
		\centering
		\begin{tabular}{lcc}
			& \textbf{Reference} & \\
			\textbf{Prediction} & nonspam & spam \\
			nonspam & 675 & 22 \\
			spam & 53 & 400 \\
		\end{tabular}
		\caption{Confusion Matrix - LDA, all features}
		\label{tab:confusion_matrix_lda1_log}
	\end{table}
	
	\begin{table}[h]
		\centering
		\begin{tabular}{lcc}
			& \textbf{Reference} & \\
			\textbf{Prediction} & nonspam & spam \\
			nonspam & 558 & 139 \\
			spam & 25 & 428 \\
		\end{tabular}
		\caption{Confusion Matrix - QDA, all features}
		\label{tab:confusion_matrix_qda1_log}
	\end{table}
	
	We see that this transformation helped our algorithm to obtain better accuracy: $0.9348$ for LDA and $0.8574$ for QDA. From that we conclude that other feature subsets will be tested for data after logarithmic transformation.
	
	In the next part we try to build predictions using only some subsets of features. We start with using either only variables representing frequencies and those describing capital letters statistics.
	
	\subsection*{Frequency features}
	
	We again provide the confusion matrices that describes the classification methods: Table \ref{tab:confusion_matrix_lda2} add Table \ref{tab:confusion_matrix_qda2}.
	
	\begin{table}[h]
		\centering
		\begin{tabular}{lcc}
			& \textbf{Reference} & \\
			\textbf{Prediction} & nonspam & spam \\
			nonspam & 670 & 27 \\
			spam & 56 & 397 \\
		\end{tabular}
		\caption{Confusion Matrix - LDA, frequency features}
		\label{tab:confusion_matrix_lda2}
	\end{table}
	
	\begin{table}[h]
		\centering
		\begin{tabular}{lcc}
			& \textbf{Reference} & \\
			\textbf{Prediction} & nonspam & spam \\
			nonspam & 552 & 145 \\
			spam & 24 & 429 \\
		\end{tabular}
		\caption{Confusion Matrix - QDA, frequency features}
		\label{tab:confusion_matrix_qda2}
	\end{table}
	
	Above results exhibit accuracy $0.9278$ for LDA and $0.853$ for QDA. We also observe similar disproportion in characteristics as for the previous case with $0.9363$ sensitivity and $0.9229$ specificity for LDA and respectively $0.7474$ and $0.9583$ for QDA. As we see elimination of features describing capital letters didn't have an enormous impact on discriminant analysis results.
	
	\subsection*{Capital features}
	
	 This time we take into account only three features containing information about capital letters and get Tables \ref{tab:confusion_matrix_lda3} and \ref{tab:confusion_matrix_qda3}.
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{table}[h]
		\centering
		\begin{tabular}{lcc}
			& \textbf{Reference} & \\
			\textbf{Prediction} & nonspam & spam \\
			nonspam & 595 & 102 \\
			spam & 208 & 245 \\
		\end{tabular}
		\caption{Confusion Matrix - LDA, capital features}
		\label{tab:confusion_matrix_lda3}
	\end{table}
	
	\begin{table}[h]
		\centering
		\begin{tabular}{lcc}
			& \textbf{Reference} & \\
			\textbf{Prediction} & nonspam & spam \\
			nonspam & 619 & 78 \\
			spam & 247 & 206 \\
		\end{tabular}
		\caption{Confusion Matrix - QDA, capital features}
		\label{tab:confusion_matrix_qda3}
	\end{table}
	 
	 Here we obtain much worse accuracy: $0.7304$ for LDA and $0.7148$ - QDA. What's interesting is that this time the disproportion between wrong classification of different discriminant analysis algorithms is not that high and both methods are more likely to incorrectly classify spam as nonspam. This results in sensitivity $0.7061$ and specificity $0.7410$ for LDA and $0.7254$, $0.7148$ respectively for QDA.
	 
	 \subsection*{Features highly correlated with type}
	 
	 LDA and QDA results are presented in Table \ref{tab:confusion_matrix_lda4} and Table \ref{tab:confusion_matrix_qda4}.
	 
	\begin{table}[h]
		\centering
		\begin{tabular}{lcc}
			& \textbf{Reference} & \\
			\textbf{Prediction} & nonspam & spam \\
			nonspam & 666 & 31 \\
			spam & 91 & 362 \\
		\end{tabular}
		\caption{Confusion Matrix - LDA, features correlated with type}
		\label{tab:confusion_matrix_lda4}
	\end{table}
	
	\begin{table}[h]
		\centering
		\begin{tabular}{lcc}
			& \textbf{Reference} & \\
			\textbf{Prediction} & nonspam & spam \\
			nonspam & 650 & 47 \\
			spam & 91 & 362 \\
		\end{tabular}
		\caption{Confusion Matrix - QDA, features correlated with type}
		\label{tab:confusion_matrix_qda4}
	\end{table}
	
	Despite having much less explanatory variables than when taking all frequency features we obtain similar accuracy: $0.8939$ - LDA and $0.88$ - QDA. It's also important to note that QDA accuracy is better in this case than for any other feature subset tested before. This variable selection also omits the disproportions between two analyzed discriminant analysis methods. Such property results in similar statistics for both of them. Namely $0.9211$ sensitivity and $0.8798$ specificity for LDA,  $0.8851$ and $0.8772$ respectively for QDA.
	 
	 \subsection*{features with highest average difference in frequency between types}
	 
	 Last feature selection that we test is for variables exhibiting highest difference in average frequency between types. Confusion matrices are shown in Table \ref{tab:confusion_matrix_lda5} ad Table \ref{tab:confusion_matrix_qda5}.
	 
	 \begin{table}[h]
	 	\centering
	 	\begin{tabular}{lcc}
	 		& \textbf{Reference} & \\
	 		\textbf{Prediction} & nonspam & spam \\
	 		nonspam & 644 & 53 \\
	 		spam & 75 & 378 \\
	 	\end{tabular}
	 	\caption{Confusion Matrix - LDA, features with highest average difference in}
	 	\label{tab:confusion_matrix_lda5}
	 \end{table}
	 
	 \begin{table}[h]
	 	\centering
	 	\begin{tabular}{lcc}
	 		& \textbf{Reference} & \\
	 		\textbf{Prediction} & nonspam & spam \\
	 		nonspam & 478 & 219 \\
	 		spam & 19 & 434 \\
	 	\end{tabular}
	 	\caption{Confusion Matrix - QDA, features with highest average difference in}
	 	\label{tab:confusion_matrix_qda5}
	 \end{table}
	 
	 In this case accuracy of LDA is still comparable to other feature subsets, namely $0.8887$, however for QDA we get $0.793$. It means that this feature subset didn't have much impact on linear algorithm, but for quadratic one it displays much worse accuracy. Also for QDA the disproportion between sensitivity and specificity is significant: $0.6646$ and $0.9618$ respectively. For LDA the disproportion is almost invisible: $0.877$ - sensitivity and $0.8957$ - specificity.
	 
	 
	 

\end{document}